{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edf716a",
   "metadata": {},
   "source": [
    "## 0. Importación y ejecución del entorno\n",
    "\n",
    "En este notebook se ilustra un flujo RAG (Retrieval-Augmented Generation) usando:\n",
    "- Carga de documentos: `langchain_community.document_loaders` (PDF/text).\n",
    "- Separación de texto en chunks: `langchain_text_splitters`.\n",
    "- Generación de embeddings: `langchain_ollama` (Ollama Embeddings).\n",
    "- Almacenamiento vectorial: `langchain_chroma` (Chroma).\n",
    "- LLM para generación: `langchain_google_genai` (Gemini) como ejemplo; también se muestra cómo configurar otros LLMs.\n",
    "\n",
    "Objetivos de la sección: explicar las dependencias, comprobar las importaciones y dejar claro cómo configurar la variable de entorno `API_KEY` para los LLMs.\n",
    "\n",
    "Notas prácticas:\n",
    "- Si no tienes el PDF de ejemplo, coloca un archivo llamado `documento_ejemplo.pdf` o modifica la ruta en la celda de carga.\n",
    "- Para reproducibilidad conviene crear un `persist_directory` cuando se crea la colección Chroma (ver sección 4).\n",
    "- Antes de ejecutar las celdas que usan la API pública, verifica que `API_KEY` esté definida en el entorno o en un archivo `.env` (se usa `python-dotenv` en el notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c241ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, PDFPlumberLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key =os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab85236a",
   "metadata": {},
   "source": [
    "## 1. Implementación práctica de RAG\n",
    "\n",
    "Esta sección contiene el flujo mínimo para montar RAG: carga del documento, fragmentación en chunks, generación de embeddings, construcción del vectorstore (Chroma) y consulta con un LLM que reescribe o responde usando el contexto recuperado.\n",
    "\n",
    "Qué hace el código adjunto:\n",
    "- `cargar_pdf(ruta_pdf)`: carga un PDF y devuelve una lista de `Document` por página (usa `PyPDFLoader`).\n",
    "- `partir_texto(docs_or_text)`: normaliza y fragmenta el texto usando `CharacterTextSplitter` (parámetros por defecto: `chunk_size=100`, `chunk_overlap=20`).\n",
    "- `crear_embeddings()`: instancia `OllamaEmbeddings` (modelo por defecto `nomic-embed-text`).\n",
    "- `crear_vectorstore(docs, embedding)`: crea una colección Chroma en memoria; recomendamos añadir `persist_directory` para guardarla entre sesiones.\n",
    "- `recuperar_contexto(pregunta, vectorstore)`: busca los 3 documentos más similares y concatena su contenido como contexto para el LLM.\n",
    "- `responder_llm(pregunta, contexto)`: envía un prompt al LLM y devuelve la respuesta (ej.: Gemini).\n",
    "\n",
    "Limitaciones y recomendaciones prácticas:\n",
    "- Asegúrate de usar un documento de ejemplo (p. ej. `documento_ejemplo.pdf`) en el repo o cambia la ruta.\n",
    "- Para mejorar trazabilidad, guarda `page_content` y `metadata` de cada chunk en Chroma con un `chunk_id`.\n",
    "- Mantén el `temperature` del LLM bajo (0.0–0.3) para respuestas basadas en contexto, y pide al LLM que cite fragmentos si necesitas fuentes.\n",
    "\n",
    "Siguiente paso en el notebook: se mostrarán experimentos para comparar embeddings y LLMs, medir efectos del splitter y ejemplos de prompts refinados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56dadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_pdf(ruta_pdf: str):\n",
    "    loader = PyPDFLoader(ruta_pdf)\n",
    "    docs = loader.load()  # documentos por página con metadata\n",
    "    return docs\n",
    "\n",
    "text = cargar_pdf(\"seguridad.pdf\")\n",
    "\n",
    "def partir_texto(docs_or_text):\n",
    "    splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "    \n",
    "    # aceptar tanto lista de Documents como texto plano\n",
    "    if isinstance(docs_or_text, list):\n",
    "        texts = [d.page_content for d in docs_or_text]\n",
    "    else:\n",
    "        texts = [docs_or_text]\n",
    "        \n",
    "    docs = splitter.create_documents(texts)\n",
    "    return docs\n",
    "\n",
    "docs = partir_texto(text)\n",
    "\n",
    "def crear_embeddings():\n",
    "    embedding = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text:latest\",\n",
    ")\n",
    "\n",
    "\n",
    "embedding = crear_embeddings()\n",
    "\n",
    "def crear_vectorstore(docs, embedding):\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding,\n",
    "        collection_name=\"base_rag_mem_384\"\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "vectorstore = crear_vectorstore(docs, embedding)\n",
    "\n",
    "def recuperar_contexto(pregunta, vectorstore):\n",
    "    docs_rel = vectorstore.similarity_search(pregunta, k=3)\n",
    "    contexto = \"\\n\".join([d.page_content for d in docs_rel])\n",
    "    return contexto\n",
    "\n",
    "pregunta = \"¿Que es el Command Injection?\"\n",
    "contexto = recuperar_contexto(pregunta, vectorstore)\n",
    "\n",
    "def responder_llm(pregunta, contexto):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0.3,\n",
    "        api_key=os.getenv(\"API_KEY\")\n",
    "    )\n",
    "    prompt = (\n",
    "        f\"Eres un asistente sobre LangChain.\\n\"\n",
    "        f\"Responde usando sólo este contexto:\\n{contexto}\\n\"\n",
    "        f\"Pregunta: {pregunta}\"\n",
    "    )\n",
    "    respuesta = llm.invoke(prompt)\n",
    "    return respuesta.content\n",
    "\n",
    "respuesta = responder_llm(pregunta, contexto)\n",
    "print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee18cbe",
   "metadata": {},
   "source": [
    "## 2. Exploración de embeddings y LLMs (breve)\n",
    "\n",
    "Qué probar (rápido):\n",
    "- Generar embeddings con 2–3 modelos (ej. `nomic-embed-text`, `all-minilm`).\n",
    "- Crear colecciones Chroma separadas y buscar con 3 preguntas simples.\n",
    "- Medir `recall@k` (k=1,3) y tiempo de respuesta.\n",
    "\n",
    "Consejo: guarda los resultados en un dict o CSV para comparar fácilmente. Si un modelo no está disponible, cámbialo por uno existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae116b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Varios embeddings\n",
    "embedding_models = [\"nomic-embed-text\", \"mxbai-embed-large\", \"all-minilm\"]\n",
    "\n",
    "def probar_embeddings(pregunta):\n",
    "    resultados = {}\n",
    "    for model in embedding_models:\n",
    "        emb = OllamaEmbeddings(model=model)\n",
    "        vs = Chroma.from_documents(docs, emb, collection_name=f\"prueba_{model}\")\n",
    "        docs_rel = vs.similarity_search(pregunta, k=3)\n",
    "        resultados[model] = [d.page_content for d in docs_rel]\n",
    "    return resultados\n",
    "\n",
    "res_emb = probar_embeddings(\"¿que es Injection (familia)?\")\n",
    "print(res_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_models = {\n",
    "    \"gemini-2.5-flash\": ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.2, api_key=os.getenv(\"API_KEY\")),\n",
    "    \"gemini-2.5-pro\": ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0.2, api_key=os.getenv(\"API_KEY\")),\n",
    "}\n",
    "\n",
    "def probar_llms(pregunta, contexto):\n",
    "    resultados = {}\n",
    "    for nombre, llm in llm_models.items():\n",
    "        prompt = f\"Contexto:\\n{contexto}\\nPregunta: {pregunta}\"\n",
    "        respuesta = llm.invoke(prompt)\n",
    "        resultados[nombre] = respuesta.content\n",
    "    return resultados\n",
    "\n",
    "ctx = recuperar_contexto(\"Dime que es Cross-Site Scripting\", vectorstore)\n",
    "res_llm = probar_llms(\"Dime que es Cross-Site Scripting\", ctx)\n",
    "print(res_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d1ad6",
   "metadata": {},
   "source": [
    "### 3. Optimización del separador (investigación)\n",
    "\n",
    "Resumen de resultados y recomendaciones breves:\n",
    "- CharacterTextSplitter: simple y efectivo como punto de partida. Recomendado para artículos y notas (chunk_size 80–150, overlap 10–20).\n",
    "- RecursiveCharacterTextSplitter: mantiene estructura (secciones → párrafos → oraciones). Mejor para manuales o documentos largos (chunk_size 150–300, overlap 20–40).\n",
    "- TokenTextSplitter: divide por tokens del modelo; útil si necesitas respetar límites de tokens del LLM o medir coste por token. Ideal cuando trabajas con límites estrictos.\n",
    "- SentenceSplitter / split por oraciones: evita cortar frases; bueno para textos legales o narrativos donde la oración completa importa.\n",
    "- Splitters basados en reglas (regex/Markdown): los mejores para código, README o tablas (ej.: MarkdownTextSplitter para respetar encabezados y bloques de código).\n",
    "\n",
    "Conclusión práctica: elegir según el tipo de documento — para empezar usa Character/Recursive; para código o markdown usa splitters por estructura; si el modelo impone límites, usa TokenTextSplitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a47c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probar_splitter_sizes(text):\n",
    "    tamaños = [(80, 10), (150, 20), (300, 40)]\n",
    "    resultados = {}\n",
    "\n",
    "    # Normalizar la entrada a una lista de strings que CharacterTextSplitter espera\n",
    "    if isinstance(text, list):\n",
    "        # Si es una lista de Document objetos, extraer page_content\n",
    "        if len(text) > 0 and hasattr(text[0], \"page_content\"):\n",
    "            texts = [d.page_content for d in text]\n",
    "        else:\n",
    "            texts = [str(t) for t in text]\n",
    "    else:\n",
    "        texts = [text]\n",
    "\n",
    "    for size, overlap in tamaños:\n",
    "        splitter = CharacterTextSplitter(chunk_size=size, chunk_overlap=overlap)\n",
    "        docs = splitter.create_documents(texts)\n",
    "        n_docs = len(docs) if docs else 0\n",
    "        avg_len = sum(len(d.page_content) for d in docs)/n_docs if n_docs else 0\n",
    "        resultados[(size, overlap)] = {\"n_chunks\": n_docs, \"avg_len\": avg_len}\n",
    "    return resultados\n",
    "\n",
    "split_stats = probar_splitter_sizes(text)\n",
    "print(split_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9835a31",
   "metadata": {},
   "source": [
    "## 4. Gestión de bases vectoriales (investigación)\n",
    "\n",
    "Comparación corta (ventajas / desventajas):\n",
    "- Chroma: fácil de usar y buena persistencia local. Ventaja: integración rápida en notebooks. Desventaja: menos optimizada para escala masiva. Caso: prototipos y PoC.\n",
    "- FAISS: muy rápida en búsquedas locales y flexible en índices. Ventaja: rendimiento. Desventaja: metadata y persistencia requieren manejo extra. Caso: sistemas locales que requieren velocidad.\n",
    "- Qdrant: soporte nativo para metadata y filtros, API REST. Ventaja: filtrado y escalado moderado. Desventaja: requiere servicio. Caso: apps con filtrado por metadata.\n",
    "- Milvus: diseñada para grandes volúmenes y escalabilidad. Ventaja: escala horizontal. Desventaja: complejidad operativa. Caso: grandes datasets en producción.\n",
    "- Pinecone / Weaviate (gestionados): ventaja: servicio completo y features; desventaja: coste y latencia de red. Caso: producción sin gestionar infra.\n",
    "\n",
    "Recomendación práctica: empezar con Chroma (persist_directory) y probar FAISS si necesitas más velocidad local; migrar a Qdrant/Milvus/Pinecone según necesidad de metadata y escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_con_metadatos(docs):\n",
    "    for i, d in enumerate(docs):\n",
    "        d.metadata = {\"chunk_id\": i, \"tema\": \"Command Injection\"}\n",
    "    return docs\n",
    "\n",
    "docs_meta = chunks_con_metadatos(docs)\n",
    "\n",
    "vect_meta = Chroma.from_documents(\n",
    "    documents=docs_meta,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"meta_rag\"\n",
    ")\n",
    "\n",
    "filtrados = vect_meta.similarity_search(\"¿Command Injection?\", k=3, filter={\"tema\": \"Command Injection\"})\n",
    "for d in filtrados:\n",
    "    print(d.metadata, d.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6856f68",
   "metadata": {},
   "source": [
    "## 5. Refinamiento de prompts (rápido)\n",
    "\n",
    "Tips sencillos:\n",
    "- Pide respuestas cortas y pide fuentes: 'Responde en bullets y al final pon Fuentes'.\n",
    "- Baja `temperature` (0.0–0.3) si quieres respuestas fieles al contexto.\n",
    "- Si quieres parsear fácil: pide salida JSON con `answer` y `sources`.\n",
    "\n",
    "Prueba: Ejecuta 2–3 prompts distintos con la misma pregunta y compara cuál da mejores fuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt con bullets\n",
    "prompt_bullets = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Responde con bullets y cita fragmentos. Si no está, di 'No está en el documento'.\"),\n",
    "    (\"human\", \"Pregunta: {question}\\nContexto:{context}\\nFormato: - punto 1 [Fuente]\")\n",
    "])\n",
    "\n",
    "def responder_bullets(pregunta, contexto):\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0, api_key=os.getenv(\"API_KEY\"))\n",
    "    formatted_prompt = f\"{pregunta}\\nContexto:{contexto}\"\n",
    "    return llm.invoke(formatted_prompt).content\n",
    "\n",
    "print(responder_bullets(\"Explicame que es Insecure Direct Object Reference (IDOR)\", contexto))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43010fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt con verificación\n",
    "prompt_verif = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Responde breve y agrega al final 'Verificado con contexto'.\"),\n",
    "    (\"human\", \"Pregunta: {question}\\nContexto:{context}\")\n",
    "])\n",
    "\n",
    "def responder_verif(pregunta, contexto):\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0.1, api_key=os.getenv(\"API_KEY\"))\n",
    "    formatted_prompt = f\"{pregunta}\\nContexto:{contexto}\"\n",
    "    return llm.invoke(formatted_prompt).content\n",
    "\n",
    "print(responder_verif(\"Dime ejemplos sobre Insecure Direct Object Reference (IDOR)\", contexto))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt con fuentes\n",
    "prompt_fuentes = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Al final incluye 'Fuentes' con los fragmentos usados.\"),\n",
    "    (\"human\", \"Pregunta: {question}\\nContexto:{context}\")\n",
    "])\n",
    "\n",
    "def responder_fuentes(pregunta, contexto):\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.2, api_key=os.getenv(\"API_KEY\"))\n",
    "    formatted_prompt = f\"{pregunta}\\nContexto:{contexto}\"\n",
    "    return llm.invoke(formatted_prompt).content\n",
    "\n",
    "print(responder_fuentes(\"Resume el documento en 2 frases.\", contexto))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
